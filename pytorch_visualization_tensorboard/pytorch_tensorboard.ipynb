{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, models, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('runs/first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root='.', download=True, train=True, transform=transforms.ToTensor())\n",
    "valid_dataset = datasets.MNIST(root='.', download=True, train=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(dataset=valid_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\pytorch-course\\torchenv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "E:\\pytorch-course\\torchenv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "model.fc = nn.Linear(in_features=512, out_features=10, bias=True)\n",
    "# model.fc = nn.Linear(in_features=2048, out_features=10, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%self.1 : __torch__.torchvision.models.resnet.ResNet,\n",
      "      %x.1 : Float(32, 1, 28, 28, strides=[784, 784, 28, 1], requires_grad=0, device=cuda:0)):\n",
      "  %fc : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"fc\"](%self.1)\n",
      "  %avgpool : __torch__.torch.nn.modules.pooling.AdaptiveAvgPool2d = prim::GetAttr[name=\"avgpool\"](%self.1)\n",
      "  %layer4 : __torch__.torch.nn.modules.container.___torch_mangle_58.Sequential = prim::GetAttr[name=\"layer4\"](%self.1)\n",
      "  %layer3 : __torch__.torch.nn.modules.container.___torch_mangle_42.Sequential = prim::GetAttr[name=\"layer3\"](%self.1)\n",
      "  %layer2 : __torch__.torch.nn.modules.container.___torch_mangle_26.Sequential = prim::GetAttr[name=\"layer2\"](%self.1)\n",
      "  %layer1 : __torch__.torch.nn.modules.container.Sequential = prim::GetAttr[name=\"layer1\"](%self.1)\n",
      "  %maxpool : __torch__.torch.nn.modules.pooling.MaxPool2d = prim::GetAttr[name=\"maxpool\"](%self.1)\n",
      "  %relu.1 : __torch__.torch.nn.modules.activation.ReLU = prim::GetAttr[name=\"relu\"](%self.1)\n",
      "  %bn1.1 : __torch__.torch.nn.modules.batchnorm.BatchNorm2d = prim::GetAttr[name=\"bn1\"](%self.1)\n",
      "  %conv1.1 : __torch__.torch.nn.modules.conv.Conv2d = prim::GetAttr[name=\"conv1\"](%self.1)\n",
      "  %1223 : bool = prim::Constant[value=1](), scope: __module.conv1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %1224 : int = prim::Constant[value=0](), scope: __module.conv1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %1225 : bool = prim::Constant[value=0](), scope: __module.conv1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %1226 : int = prim::Constant[value=1](), scope: __module.conv1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %1227 : int = prim::Constant[value=3](), scope: __module.conv1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %1228 : int = prim::Constant[value=2](), scope: __module.conv1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %1229 : NoneType = prim::Constant(), scope: __module.conv1\n",
      "  %weight.81 : Tensor = prim::GetAttr[name=\"weight\"](%conv1.1)\n",
      "  %1231 : int[] = prim::ListConstruct(%1228, %1228), scope: __module.conv1\n",
      "  %1232 : int[] = prim::ListConstruct(%1227, %1227), scope: __module.conv1\n",
      "  %1233 : int[] = prim::ListConstruct(%1226, %1226), scope: __module.conv1\n",
      "  %1234 : int[] = prim::ListConstruct(%1224, %1224), scope: __module.conv1\n",
      "  %input.1 : Float(32, 64, 14, 14, strides=[12544, 196, 14, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%x.1, %weight.81, %1229, %1231, %1232, %1233, %1225, %1234, %1226, %1225, %1225, %1223, %1223), scope: __module.conv1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %1236 : bool = prim::Constant[value=1](), scope: __module.bn1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:2435:0\n",
      "  %1237 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.bn1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:2435:0\n",
      "  %1238 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bn1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:2435:0\n",
      "  %1239 : bool = prim::Constant[value=0](), scope: __module.bn1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:2435:0\n",
      "  %running_var.41 : Tensor = prim::GetAttr[name=\"running_var\"](%bn1.1)\n",
      "  %running_mean.41 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn1.1)\n",
      "  %bias.41 : Tensor = prim::GetAttr[name=\"bias\"](%bn1.1)\n",
      "  %weight.83 : Tensor = prim::GetAttr[name=\"weight\"](%bn1.1)\n",
      "  %input.3 : Float(32, 64, 14, 14, strides=[12544, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.1, %weight.83, %bias.41, %running_mean.41, %running_var.41, %1239, %1238, %1237, %1236), scope: __module.bn1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:2435:0\n",
      "  %input.5 : Float(32, 64, 14, 14, strides=[12544, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.3), scope: __module.relu # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:1453:0\n",
      "  %1246 : bool = prim::Constant[value=0](), scope: __module.maxpool # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:780:0\n",
      "  %1247 : int = prim::Constant[value=1](), scope: __module.maxpool # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:780:0\n",
      "  %1248 : int = prim::Constant[value=2](), scope: __module.maxpool # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:780:0\n",
      "  %1249 : int = prim::Constant[value=3](), scope: __module.maxpool # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:780:0\n",
      "  %1250 : int[] = prim::ListConstruct(%1249, %1249), scope: __module.maxpool\n",
      "  %1251 : int[] = prim::ListConstruct(%1248, %1248), scope: __module.maxpool\n",
      "  %1252 : int[] = prim::ListConstruct(%1247, %1247), scope: __module.maxpool\n",
      "  %1253 : int[] = prim::ListConstruct(%1247, %1247), scope: __module.maxpool\n",
      "  %input.7 : Float(32, 64, 7, 7, strides=[3136, 49, 7, 1], requires_grad=1, device=cuda:0) = aten::max_pool2d(%input.5, %1250, %1251, %1252, %1253, %1246), scope: __module.maxpool # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:780:0\n",
      "  %1255 : bool = prim::Constant[value=1](), scope: __module.layer1/__module.layer1.0/__module.layer1.0.conv1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %1256 : int = prim::Constant[value=0](), scope: __module.layer1/__module.layer1.0/__module.layer1.0.conv1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %1257 : bool = prim::Constant[value=0](), scope: __module.layer1/__module.layer1.0/__module.layer1.0.conv1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %1258 : int = prim::Constant[value=1](), scope: __module.layer1/__module.layer1.0/__module.layer1.0.conv1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %1259 : NoneType = prim::Constant(), scope: __module.layer1/__module.layer1.0/__module.layer1.0.conv1\n",
      "  %1260 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.layer1/__module.layer1.0/__module.layer1.0.bn1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:2435:0\n",
      "  %1261 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer1/__module.layer1.0/__module.layer1.0.bn1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:2435:0\n",
      "  %_1.1 : __torch__.torchvision.models.resnet.___torch_mangle_10.BasicBlock = prim::GetAttr[name=\"1\"](%layer1)\n",
      "  %_0.1 : __torch__.torchvision.models.resnet.BasicBlock = prim::GetAttr[name=\"0\"](%layer1)\n",
      "  %bn2.1 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_4.BatchNorm2d = prim::GetAttr[name=\"bn2\"](%_0.1)\n",
      "  %conv2.1 : __torch__.torch.nn.modules.conv.___torch_mangle_3.Conv2d = prim::GetAttr[name=\"conv2\"](%_0.1)\n",
      "  %relu.3 : __torch__.torch.nn.modules.activation.___torch_mangle_2.ReLU = prim::GetAttr[name=\"relu\"](%_0.1)\n",
      "  %bn1.3 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_1.BatchNorm2d = prim::GetAttr[name=\"bn1\"](%_0.1)\n",
      "  %conv1.3 : __torch__.torch.nn.modules.conv.___torch_mangle_0.Conv2d = prim::GetAttr[name=\"conv1\"](%_0.1)\n",
      "  %weight.85 : Tensor = prim::GetAttr[name=\"weight\"](%conv1.3)\n",
      "  %1270 : int[] = prim::ListConstruct(%1258, %1258), scope: __module.layer1/__module.layer1.0/__module.layer1.0.conv1\n",
      "  %1271 : int[] = prim::ListConstruct(%1258, %1258), scope: __module.layer1/__module.layer1.0/__module.layer1.0.conv1\n",
      "  %1272 : int[] = prim::ListConstruct(%1258, %1258), scope: __module.layer1/__module.layer1.0/__module.layer1.0.conv1\n",
      "  %1273 : int[] = prim::ListConstruct(%1256, %1256), scope: __module.layer1/__module.layer1.0/__module.layer1.0.conv1\n",
      "  %input.9 : Float(32, 64, 7, 7, strides=[3136, 49, 7, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.7, %weight.85, %1259, %1270, %1271, %1272, %1257, %1273, %1258, %1257, %1257, %1255, %1255), scope: __module.layer1/__module.layer1.0/__module.layer1.0.conv1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %running_var.43 : Tensor = prim::GetAttr[name=\"running_var\"](%bn1.3)\n",
      "  %running_mean.43 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn1.3)\n",
      "  %bias.43 : Tensor = prim::GetAttr[name=\"bias\"](%bn1.3)\n",
      "  %weight.87 : Tensor = prim::GetAttr[name=\"weight\"](%bn1.3)\n",
      "  %input.11 : Float(32, 64, 7, 7, strides=[3136, 49, 7, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.9, %weight.87, %bias.43, %running_mean.43, %running_var.43, %1257, %1261, %1260, %1255), scope: __module.layer1/__module.layer1.0/__module.layer1.0.bn1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:2435:0\n",
      "  %input.13 : Float(32, 64, 7, 7, strides=[3136, 49, 7, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.11), scope: __module.layer1/__module.layer1.0/__module.layer1.0.relu # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:1453:0\n",
      "  %weight.89 : Tensor = prim::GetAttr[name=\"weight\"](%conv2.1)\n",
      "  %1282 : int[] = prim::ListConstruct(%1258, %1258), scope: __module.layer1/__module.layer1.0/__module.layer1.0.conv2\n",
      "  %1283 : int[] = prim::ListConstruct(%1258, %1258), scope: __module.layer1/__module.layer1.0/__module.layer1.0.conv2\n",
      "  %1284 : int[] = prim::ListConstruct(%1258, %1258), scope: __module.layer1/__module.layer1.0/__module.layer1.0.conv2\n",
      "  %1285 : int[] = prim::ListConstruct(%1256, %1256), scope: __module.layer1/__module.layer1.0/__module.layer1.0.conv2\n",
      "  %input.15 : Float(32, 64, 7, 7, strides=[3136, 49, 7, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.13, %weight.89, %1259, %1282, %1283, %1284, %1257, %1285, %1258, %1257, %1257, %1255, %1255), scope: __module.layer1/__module.layer1.0/__module.layer1.0.conv2 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %running_var.45 : Tensor = prim::GetAttr[name=\"running_var\"](%bn2.1)\n",
      "  %running_mean.45 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn2.1)\n",
      "  %bias.45 : Tensor = prim::GetAttr[name=\"bias\"](%bn2.1)\n",
      "  %weight.91 : Tensor = prim::GetAttr[name=\"weight\"](%bn2.1)\n",
      "  %out.1 : Float(32, 64, 7, 7, strides=[3136, 49, 7, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.15, %weight.91, %bias.45, %running_mean.45, %running_var.45, %1257, %1261, %1260, %1255), scope: __module.layer1/__module.layer1.0/__module.layer1.0.bn2 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:2435:0\n",
      "  %input.17 : Float(32, 64, 7, 7, strides=[3136, 49, 7, 1], requires_grad=1, device=cuda:0) = aten::add_(%out.1, %input.7, %1258), scope: __module.layer1/__module.layer1.0 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torchvision\\models\\resnet.py:96:0\n",
      "  %input.19 : Float(32, 64, 7, 7, strides=[3136, 49, 7, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.17), scope: __module.layer1/__module.layer1.0/__module.layer1.0.relu # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:1453:0\n",
      "  %bn2.3 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_9.BatchNorm2d = prim::GetAttr[name=\"bn2\"](%_1.1)\n",
      "  %conv2.3 : __torch__.torch.nn.modules.conv.___torch_mangle_8.Conv2d = prim::GetAttr[name=\"conv2\"](%_1.1)\n",
      "  %relu.5 : __torch__.torch.nn.modules.activation.___torch_mangle_7.ReLU = prim::GetAttr[name=\"relu\"](%_1.1)\n",
      "  %bn1.5 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_6.BatchNorm2d = prim::GetAttr[name=\"bn1\"](%_1.1)\n",
      "  %conv1.5 : __torch__.torch.nn.modules.conv.___torch_mangle_5.Conv2d = prim::GetAttr[name=\"conv1\"](%_1.1)\n",
      "  %weight.93 : Tensor = prim::GetAttr[name=\"weight\"](%conv1.5)\n",
      "  %1300 : int[] = prim::ListConstruct(%1258, %1258), scope: __module.layer1/__module.layer1.1/__module.layer1.1.conv1\n",
      "  %1301 : int[] = prim::ListConstruct(%1258, %1258), scope: __module.layer1/__module.layer1.1/__module.layer1.1.conv1\n",
      "  %1302 : int[] = prim::ListConstruct(%1258, %1258), scope: __module.layer1/__module.layer1.1/__module.layer1.1.conv1\n",
      "  %1303 : int[] = prim::ListConstruct(%1256, %1256), scope: __module.layer1/__module.layer1.1/__module.layer1.1.conv1\n",
      "  %input.21 : Float(32, 64, 7, 7, strides=[3136, 49, 7, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.19, %weight.93, %1259, %1300, %1301, %1302, %1257, %1303, %1258, %1257, %1257, %1255, %1255), scope: __module.layer1/__module.layer1.1/__module.layer1.1.conv1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %running_var.47 : Tensor = prim::GetAttr[name=\"running_var\"](%bn1.5)\n",
      "  %running_mean.47 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn1.5)\n",
      "  %bias.47 : Tensor = prim::GetAttr[name=\"bias\"](%bn1.5)\n",
      "  %weight.95 : Tensor = prim::GetAttr[name=\"weight\"](%bn1.5)\n",
      "  %input.23 : Float(32, 64, 7, 7, strides=[3136, 49, 7, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.21, %weight.95, %bias.47, %running_mean.47, %running_var.47, %1257, %1261, %1260, %1255), scope: __module.layer1/__module.layer1.1/__module.layer1.1.bn1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:2435:0\n",
      "  %input.25 : Float(32, 64, 7, 7, strides=[3136, 49, 7, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.23), scope: __module.layer1/__module.layer1.1/__module.layer1.1.relu # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:1453:0\n",
      "  %weight.97 : Tensor = prim::GetAttr[name=\"weight\"](%conv2.3)\n",
      "  %1312 : int[] = prim::ListConstruct(%1258, %1258), scope: __module.layer1/__module.layer1.1/__module.layer1.1.conv2\n",
      "  %1313 : int[] = prim::ListConstruct(%1258, %1258), scope: __module.layer1/__module.layer1.1/__module.layer1.1.conv2\n",
      "  %1314 : int[] = prim::ListConstruct(%1258, %1258), scope: __module.layer1/__module.layer1.1/__module.layer1.1.conv2\n",
      "  %1315 : int[] = prim::ListConstruct(%1256, %1256), scope: __module.layer1/__module.layer1.1/__module.layer1.1.conv2\n",
      "  %input.27 : Float(32, 64, 7, 7, strides=[3136, 49, 7, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.25, %weight.97, %1259, %1312, %1313, %1314, %1257, %1315, %1258, %1257, %1257, %1255, %1255), scope: __module.layer1/__module.layer1.1/__module.layer1.1.conv2 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %running_var.49 : Tensor = prim::GetAttr[name=\"running_var\"](%bn2.3)\n",
      "  %running_mean.49 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn2.3)\n",
      "  %bias.49 : Tensor = prim::GetAttr[name=\"bias\"](%bn2.3)\n",
      "  %weight.99 : Tensor = prim::GetAttr[name=\"weight\"](%bn2.3)\n",
      "  %out.3 : Float(32, 64, 7, 7, strides=[3136, 49, 7, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.27, %weight.99, %bias.49, %running_mean.49, %running_var.49, %1257, %1261, %1260, %1255), scope: __module.layer1/__module.layer1.1/__module.layer1.1.bn2 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:2435:0\n",
      "  %input.29 : Float(32, 64, 7, 7, strides=[3136, 49, 7, 1], requires_grad=1, device=cuda:0) = aten::add_(%out.3, %input.19, %1258), scope: __module.layer1/__module.layer1.1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torchvision\\models\\resnet.py:96:0\n",
      "  %input.31 : Float(32, 64, 7, 7, strides=[3136, 49, 7, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.29), scope: __module.layer1/__module.layer1.1/__module.layer1.1.relu # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:1453:0\n",
      "  %1324 : bool = prim::Constant[value=1](), scope: __module.layer2/__module.layer2.0/__module.layer2.0.conv1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %1325 : int = prim::Constant[value=0](), scope: __module.layer2/__module.layer2.0/__module.layer2.0.conv1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %1326 : bool = prim::Constant[value=0](), scope: __module.layer2/__module.layer2.0/__module.layer2.0.conv1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %1327 : int = prim::Constant[value=1](), scope: __module.layer2/__module.layer2.0/__module.layer2.0.conv1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %1328 : int = prim::Constant[value=2](), scope: __module.layer2/__module.layer2.0/__module.layer2.0.conv1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %1329 : NoneType = prim::Constant(), scope: __module.layer2/__module.layer2.0/__module.layer2.0.conv1\n",
      "  %1330 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.layer2/__module.layer2.0/__module.layer2.0.bn1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:2435:0\n",
      "  %1331 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer2/__module.layer2.0/__module.layer2.0.bn1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:2435:0\n",
      "  %_1.5 : __torch__.torchvision.models.resnet.___torch_mangle_25.BasicBlock = prim::GetAttr[name=\"1\"](%layer2)\n",
      "  %_0.5 : __torch__.torchvision.models.resnet.___torch_mangle_19.BasicBlock = prim::GetAttr[name=\"0\"](%layer2)\n",
      "  %downsample.1 : __torch__.torch.nn.modules.container.___torch_mangle_18.Sequential = prim::GetAttr[name=\"downsample\"](%_0.5)\n",
      "  %bn2.5 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_15.BatchNorm2d = prim::GetAttr[name=\"bn2\"](%_0.5)\n",
      "  %conv2.5 : __torch__.torch.nn.modules.conv.___torch_mangle_14.Conv2d = prim::GetAttr[name=\"conv2\"](%_0.5)\n",
      "  %relu.7 : __torch__.torch.nn.modules.activation.___torch_mangle_13.ReLU = prim::GetAttr[name=\"relu\"](%_0.5)\n",
      "  %bn1.7 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name=\"bn1\"](%_0.5)\n",
      "  %conv1.7 : __torch__.torch.nn.modules.conv.___torch_mangle_11.Conv2d = prim::GetAttr[name=\"conv1\"](%_0.5)\n",
      "  %weight.101 : Tensor = prim::GetAttr[name=\"weight\"](%conv1.7)\n",
      "  %1341 : int[] = prim::ListConstruct(%1328, %1328), scope: __module.layer2/__module.layer2.0/__module.layer2.0.conv1\n",
      "  %1342 : int[] = prim::ListConstruct(%1327, %1327), scope: __module.layer2/__module.layer2.0/__module.layer2.0.conv1\n",
      "  %1343 : int[] = prim::ListConstruct(%1327, %1327), scope: __module.layer2/__module.layer2.0/__module.layer2.0.conv1\n",
      "  %1344 : int[] = prim::ListConstruct(%1325, %1325), scope: __module.layer2/__module.layer2.0/__module.layer2.0.conv1\n",
      "  %input.33 : Float(32, 128, 4, 4, strides=[2048, 16, 4, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.31, %weight.101, %1329, %1341, %1342, %1343, %1326, %1344, %1327, %1326, %1326, %1324, %1324), scope: __module.layer2/__module.layer2.0/__module.layer2.0.conv1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %running_var.51 : Tensor = prim::GetAttr[name=\"running_var\"](%bn1.7)\n",
      "  %running_mean.51 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn1.7)\n",
      "  %bias.51 : Tensor = prim::GetAttr[name=\"bias\"](%bn1.7)\n",
      "  %weight.103 : Tensor = prim::GetAttr[name=\"weight\"](%bn1.7)\n",
      "  %input.35 : Float(32, 128, 4, 4, strides=[2048, 16, 4, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.33, %weight.103, %bias.51, %running_mean.51, %running_var.51, %1326, %1331, %1330, %1324), scope: __module.layer2/__module.layer2.0/__module.layer2.0.bn1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:2435:0\n",
      "  %input.37 : Float(32, 128, 4, 4, strides=[2048, 16, 4, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.35), scope: __module.layer2/__module.layer2.0/__module.layer2.0.relu # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:1453:0\n",
      "  %weight.105 : Tensor = prim::GetAttr[name=\"weight\"](%conv2.5)\n",
      "  %1353 : int[] = prim::ListConstruct(%1327, %1327), scope: __module.layer2/__module.layer2.0/__module.layer2.0.conv2\n",
      "  %1354 : int[] = prim::ListConstruct(%1327, %1327), scope: __module.layer2/__module.layer2.0/__module.layer2.0.conv2\n",
      "  %1355 : int[] = prim::ListConstruct(%1327, %1327), scope: __module.layer2/__module.layer2.0/__module.layer2.0.conv2\n",
      "  %1356 : int[] = prim::ListConstruct(%1325, %1325), scope: __module.layer2/__module.layer2.0/__module.layer2.0.conv2\n",
      "  %input.39 : Float(32, 128, 4, 4, strides=[2048, 16, 4, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.37, %weight.105, %1329, %1353, %1354, %1355, %1326, %1356, %1327, %1326, %1326, %1324, %1324), scope: __module.layer2/__module.layer2.0/__module.layer2.0.conv2 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %running_var.53 : Tensor = prim::GetAttr[name=\"running_var\"](%bn2.5)\n",
      "  %running_mean.53 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn2.5)\n",
      "  %bias.53 : Tensor = prim::GetAttr[name=\"bias\"](%bn2.5)\n",
      "  %weight.107 : Tensor = prim::GetAttr[name=\"weight\"](%bn2.5)\n",
      "  %out.5 : Float(32, 128, 4, 4, strides=[2048, 16, 4, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.39, %weight.107, %bias.53, %running_mean.53, %running_var.53, %1326, %1331, %1330, %1324), scope: __module.layer2/__module.layer2.0/__module.layer2.0.bn2 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:2435:0\n",
      "  %_1.3 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_17.BatchNorm2d = prim::GetAttr[name=\"1\"](%downsample.1)\n",
      "  %_0.3 : __torch__.torch.nn.modules.conv.___torch_mangle_16.Conv2d = prim::GetAttr[name=\"0\"](%downsample.1)\n",
      "  %weight.109 : Tensor = prim::GetAttr[name=\"weight\"](%_0.3)\n",
      "  %1366 : int[] = prim::ListConstruct(%1328, %1328), scope: __module.layer2/__module.layer2.0/__module.layer2.0.downsample/__module.layer2.0.downsample.0\n",
      "  %1367 : int[] = prim::ListConstruct(%1325, %1325), scope: __module.layer2/__module.layer2.0/__module.layer2.0.downsample/__module.layer2.0.downsample.0\n",
      "  %1368 : int[] = prim::ListConstruct(%1327, %1327), scope: __module.layer2/__module.layer2.0/__module.layer2.0.downsample/__module.layer2.0.downsample.0\n",
      "  %1369 : int[] = prim::ListConstruct(%1325, %1325), scope: __module.layer2/__module.layer2.0/__module.layer2.0.downsample/__module.layer2.0.downsample.0\n",
      "  %input.41 : Float(32, 128, 4, 4, strides=[2048, 16, 4, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.31, %weight.109, %1329, %1366, %1367, %1368, %1326, %1369, %1327, %1326, %1326, %1324, %1324), scope: __module.layer2/__module.layer2.0/__module.layer2.0.downsample/__module.layer2.0.downsample.0 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %running_var.55 : Tensor = prim::GetAttr[name=\"running_var\"](%_1.3)\n",
      "  %running_mean.55 : Tensor = prim::GetAttr[name=\"running_mean\"](%_1.3)\n",
      "  %bias.55 : Tensor = prim::GetAttr[name=\"bias\"](%_1.3)\n",
      "  %weight.111 : Tensor = prim::GetAttr[name=\"weight\"](%_1.3)\n",
      "  %identity.1 : Float(32, 128, 4, 4, strides=[2048, 16, 4, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.41, %weight.111, %bias.55, %running_mean.55, %running_var.55, %1326, %1331, %1330, %1324), scope: __module.layer2/__module.layer2.0/__module.layer2.0.downsample/__module.layer2.0.downsample.1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:2435:0\n",
      "  %input.43 : Float(32, 128, 4, 4, strides=[2048, 16, 4, 1], requires_grad=1, device=cuda:0) = aten::add_(%out.5, %identity.1, %1327), scope: __module.layer2/__module.layer2.0 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torchvision\\models\\resnet.py:96:0\n",
      "  %input.45 : Float(32, 128, 4, 4, strides=[2048, 16, 4, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.43), scope: __module.layer2/__module.layer2.0/__module.layer2.0.relu # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:1453:0\n",
      "  %bn2.7 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_24.BatchNorm2d = prim::GetAttr[name=\"bn2\"](%_1.5)\n",
      "  %conv2.7 : __torch__.torch.nn.modules.conv.___torch_mangle_23.Conv2d = prim::GetAttr[name=\"conv2\"](%_1.5)\n",
      "  %relu.9 : __torch__.torch.nn.modules.activation.___torch_mangle_22.ReLU = prim::GetAttr[name=\"relu\"](%_1.5)\n",
      "  %bn1.9 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name=\"bn1\"](%_1.5)\n",
      "  %conv1.9 : __torch__.torch.nn.modules.conv.___torch_mangle_20.Conv2d = prim::GetAttr[name=\"conv1\"](%_1.5)\n",
      "  %weight.113 : Tensor = prim::GetAttr[name=\"weight\"](%conv1.9)\n",
      "  %1384 : int[] = prim::ListConstruct(%1327, %1327), scope: __module.layer2/__module.layer2.1/__module.layer2.1.conv1\n",
      "  %1385 : int[] = prim::ListConstruct(%1327, %1327), scope: __module.layer2/__module.layer2.1/__module.layer2.1.conv1\n",
      "  %1386 : int[] = prim::ListConstruct(%1327, %1327), scope: __module.layer2/__module.layer2.1/__module.layer2.1.conv1\n",
      "  %1387 : int[] = prim::ListConstruct(%1325, %1325), scope: __module.layer2/__module.layer2.1/__module.layer2.1.conv1\n",
      "  %input.47 : Float(32, 128, 4, 4, strides=[2048, 16, 4, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.45, %weight.113, %1329, %1384, %1385, %1386, %1326, %1387, %1327, %1326, %1326, %1324, %1324), scope: __module.layer2/__module.layer2.1/__module.layer2.1.conv1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %running_var.57 : Tensor = prim::GetAttr[name=\"running_var\"](%bn1.9)\n",
      "  %running_mean.57 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn1.9)\n",
      "  %bias.57 : Tensor = prim::GetAttr[name=\"bias\"](%bn1.9)\n",
      "  %weight.115 : Tensor = prim::GetAttr[name=\"weight\"](%bn1.9)\n",
      "  %input.49 : Float(32, 128, 4, 4, strides=[2048, 16, 4, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.47, %weight.115, %bias.57, %running_mean.57, %running_var.57, %1326, %1331, %1330, %1324), scope: __module.layer2/__module.layer2.1/__module.layer2.1.bn1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:2435:0\n",
      "  %input.51 : Float(32, 128, 4, 4, strides=[2048, 16, 4, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.49), scope: __module.layer2/__module.layer2.1/__module.layer2.1.relu # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:1453:0\n",
      "  %weight.117 : Tensor = prim::GetAttr[name=\"weight\"](%conv2.7)\n",
      "  %1396 : int[] = prim::ListConstruct(%1327, %1327), scope: __module.layer2/__module.layer2.1/__module.layer2.1.conv2\n",
      "  %1397 : int[] = prim::ListConstruct(%1327, %1327), scope: __module.layer2/__module.layer2.1/__module.layer2.1.conv2\n",
      "  %1398 : int[] = prim::ListConstruct(%1327, %1327), scope: __module.layer2/__module.layer2.1/__module.layer2.1.conv2\n",
      "  %1399 : int[] = prim::ListConstruct(%1325, %1325), scope: __module.layer2/__module.layer2.1/__module.layer2.1.conv2\n",
      "  %input.53 : Float(32, 128, 4, 4, strides=[2048, 16, 4, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.51, %weight.117, %1329, %1396, %1397, %1398, %1326, %1399, %1327, %1326, %1326, %1324, %1324), scope: __module.layer2/__module.layer2.1/__module.layer2.1.conv2 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %running_var.59 : Tensor = prim::GetAttr[name=\"running_var\"](%bn2.7)\n",
      "  %running_mean.59 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn2.7)\n",
      "  %bias.59 : Tensor = prim::GetAttr[name=\"bias\"](%bn2.7)\n",
      "  %weight.119 : Tensor = prim::GetAttr[name=\"weight\"](%bn2.7)\n",
      "  %out.7 : Float(32, 128, 4, 4, strides=[2048, 16, 4, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.53, %weight.119, %bias.59, %running_mean.59, %running_var.59, %1326, %1331, %1330, %1324), scope: __module.layer2/__module.layer2.1/__module.layer2.1.bn2 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:2435:0\n",
      "  %input.55 : Float(32, 128, 4, 4, strides=[2048, 16, 4, 1], requires_grad=1, device=cuda:0) = aten::add_(%out.7, %input.45, %1327), scope: __module.layer2/__module.layer2.1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torchvision\\models\\resnet.py:96:0\n",
      "  %input.57 : Float(32, 128, 4, 4, strides=[2048, 16, 4, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.55), scope: __module.layer2/__module.layer2.1/__module.layer2.1.relu # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:1453:0\n",
      "  %1408 : bool = prim::Constant[value=1](), scope: __module.layer3/__module.layer3.0/__module.layer3.0.conv1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %1409 : int = prim::Constant[value=0](), scope: __module.layer3/__module.layer3.0/__module.layer3.0.conv1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %1410 : bool = prim::Constant[value=0](), scope: __module.layer3/__module.layer3.0/__module.layer3.0.conv1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %1411 : int = prim::Constant[value=1](), scope: __module.layer3/__module.layer3.0/__module.layer3.0.conv1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %1412 : int = prim::Constant[value=2](), scope: __module.layer3/__module.layer3.0/__module.layer3.0.conv1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %1413 : NoneType = prim::Constant(), scope: __module.layer3/__module.layer3.0/__module.layer3.0.conv1\n",
      "  %1414 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.layer3/__module.layer3.0/__module.layer3.0.bn1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:2435:0\n",
      "  %1415 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer3/__module.layer3.0/__module.layer3.0.bn1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:2435:0\n",
      "  %_1.9 : __torch__.torchvision.models.resnet.___torch_mangle_41.BasicBlock = prim::GetAttr[name=\"1\"](%layer3)\n",
      "  %_0.9 : __torch__.torchvision.models.resnet.___torch_mangle_35.BasicBlock = prim::GetAttr[name=\"0\"](%layer3)\n",
      "  %downsample.3 : __torch__.torch.nn.modules.container.___torch_mangle_34.Sequential = prim::GetAttr[name=\"downsample\"](%_0.9)\n",
      "  %bn2.9 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name=\"bn2\"](%_0.9)\n",
      "  %conv2.9 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name=\"conv2\"](%_0.9)\n",
      "  %relu.11 : __torch__.torch.nn.modules.activation.___torch_mangle_29.ReLU = prim::GetAttr[name=\"relu\"](%_0.9)\n",
      "  %bn1.11 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_28.BatchNorm2d = prim::GetAttr[name=\"bn1\"](%_0.9)\n",
      "  %conv1.11 : __torch__.torch.nn.modules.conv.___torch_mangle_27.Conv2d = prim::GetAttr[name=\"conv1\"](%_0.9)\n",
      "  %weight.121 : Tensor = prim::GetAttr[name=\"weight\"](%conv1.11)\n",
      "  %1425 : int[] = prim::ListConstruct(%1412, %1412), scope: __module.layer3/__module.layer3.0/__module.layer3.0.conv1\n",
      "  %1426 : int[] = prim::ListConstruct(%1411, %1411), scope: __module.layer3/__module.layer3.0/__module.layer3.0.conv1\n",
      "  %1427 : int[] = prim::ListConstruct(%1411, %1411), scope: __module.layer3/__module.layer3.0/__module.layer3.0.conv1\n",
      "  %1428 : int[] = prim::ListConstruct(%1409, %1409), scope: __module.layer3/__module.layer3.0/__module.layer3.0.conv1\n",
      "  %input.59 : Float(32, 256, 2, 2, strides=[1024, 4, 2, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.57, %weight.121, %1413, %1425, %1426, %1427, %1410, %1428, %1411, %1410, %1410, %1408, %1408), scope: __module.layer3/__module.layer3.0/__module.layer3.0.conv1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %running_var.61 : Tensor = prim::GetAttr[name=\"running_var\"](%bn1.11)\n",
      "  %running_mean.61 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn1.11)\n",
      "  %bias.61 : Tensor = prim::GetAttr[name=\"bias\"](%bn1.11)\n",
      "  %weight.123 : Tensor = prim::GetAttr[name=\"weight\"](%bn1.11)\n",
      "  %input.61 : Float(32, 256, 2, 2, strides=[1024, 4, 2, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.59, %weight.123, %bias.61, %running_mean.61, %running_var.61, %1410, %1415, %1414, %1408), scope: __module.layer3/__module.layer3.0/__module.layer3.0.bn1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:2435:0\n",
      "  %input.63 : Float(32, 256, 2, 2, strides=[1024, 4, 2, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.61), scope: __module.layer3/__module.layer3.0/__module.layer3.0.relu # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:1453:0\n",
      "  %weight.125 : Tensor = prim::GetAttr[name=\"weight\"](%conv2.9)\n",
      "  %1437 : int[] = prim::ListConstruct(%1411, %1411), scope: __module.layer3/__module.layer3.0/__module.layer3.0.conv2\n",
      "  %1438 : int[] = prim::ListConstruct(%1411, %1411), scope: __module.layer3/__module.layer3.0/__module.layer3.0.conv2\n",
      "  %1439 : int[] = prim::ListConstruct(%1411, %1411), scope: __module.layer3/__module.layer3.0/__module.layer3.0.conv2\n",
      "  %1440 : int[] = prim::ListConstruct(%1409, %1409), scope: __module.layer3/__module.layer3.0/__module.layer3.0.conv2\n",
      "  %input.65 : Float(32, 256, 2, 2, strides=[1024, 4, 2, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.63, %weight.125, %1413, %1437, %1438, %1439, %1410, %1440, %1411, %1410, %1410, %1408, %1408), scope: __module.layer3/__module.layer3.0/__module.layer3.0.conv2 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %running_var.63 : Tensor = prim::GetAttr[name=\"running_var\"](%bn2.9)\n",
      "  %running_mean.63 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn2.9)\n",
      "  %bias.63 : Tensor = prim::GetAttr[name=\"bias\"](%bn2.9)\n",
      "  %weight.127 : Tensor = prim::GetAttr[name=\"weight\"](%bn2.9)\n",
      "  %out.9 : Float(32, 256, 2, 2, strides=[1024, 4, 2, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.65, %weight.127, %bias.63, %running_mean.63, %running_var.63, %1410, %1415, %1414, %1408), scope: __module.layer3/__module.layer3.0/__module.layer3.0.bn2 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:2435:0\n",
      "  %_1.7 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_33.BatchNorm2d = prim::GetAttr[name=\"1\"](%downsample.3)\n",
      "  %_0.7 : __torch__.torch.nn.modules.conv.___torch_mangle_32.Conv2d = prim::GetAttr[name=\"0\"](%downsample.3)\n",
      "  %weight.129 : Tensor = prim::GetAttr[name=\"weight\"](%_0.7)\n",
      "  %1450 : int[] = prim::ListConstruct(%1412, %1412), scope: __module.layer3/__module.layer3.0/__module.layer3.0.downsample/__module.layer3.0.downsample.0\n",
      "  %1451 : int[] = prim::ListConstruct(%1409, %1409), scope: __module.layer3/__module.layer3.0/__module.layer3.0.downsample/__module.layer3.0.downsample.0\n",
      "  %1452 : int[] = prim::ListConstruct(%1411, %1411), scope: __module.layer3/__module.layer3.0/__module.layer3.0.downsample/__module.layer3.0.downsample.0\n",
      "  %1453 : int[] = prim::ListConstruct(%1409, %1409), scope: __module.layer3/__module.layer3.0/__module.layer3.0.downsample/__module.layer3.0.downsample.0\n",
      "  %input.67 : Float(32, 256, 2, 2, strides=[1024, 4, 2, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.57, %weight.129, %1413, %1450, %1451, %1452, %1410, %1453, %1411, %1410, %1410, %1408, %1408), scope: __module.layer3/__module.layer3.0/__module.layer3.0.downsample/__module.layer3.0.downsample.0 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %running_var.65 : Tensor = prim::GetAttr[name=\"running_var\"](%_1.7)\n",
      "  %running_mean.65 : Tensor = prim::GetAttr[name=\"running_mean\"](%_1.7)\n",
      "  %bias.65 : Tensor = prim::GetAttr[name=\"bias\"](%_1.7)\n",
      "  %weight.131 : Tensor = prim::GetAttr[name=\"weight\"](%_1.7)\n",
      "  %identity.3 : Float(32, 256, 2, 2, strides=[1024, 4, 2, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.67, %weight.131, %bias.65, %running_mean.65, %running_var.65, %1410, %1415, %1414, %1408), scope: __module.layer3/__module.layer3.0/__module.layer3.0.downsample/__module.layer3.0.downsample.1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:2435:0\n",
      "  %input.69 : Float(32, 256, 2, 2, strides=[1024, 4, 2, 1], requires_grad=1, device=cuda:0) = aten::add_(%out.9, %identity.3, %1411), scope: __module.layer3/__module.layer3.0 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torchvision\\models\\resnet.py:96:0\n",
      "  %input.71 : Float(32, 256, 2, 2, strides=[1024, 4, 2, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.69), scope: __module.layer3/__module.layer3.0/__module.layer3.0.relu # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:1453:0\n",
      "  %bn2.11 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_40.BatchNorm2d = prim::GetAttr[name=\"bn2\"](%_1.9)\n",
      "  %conv2.11 : __torch__.torch.nn.modules.conv.___torch_mangle_39.Conv2d = prim::GetAttr[name=\"conv2\"](%_1.9)\n",
      "  %relu.13 : __torch__.torch.nn.modules.activation.___torch_mangle_38.ReLU = prim::GetAttr[name=\"relu\"](%_1.9)\n",
      "  %bn1.13 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_37.BatchNorm2d = prim::GetAttr[name=\"bn1\"](%_1.9)\n",
      "  %conv1.13 : __torch__.torch.nn.modules.conv.___torch_mangle_36.Conv2d = prim::GetAttr[name=\"conv1\"](%_1.9)\n",
      "  %weight.133 : Tensor = prim::GetAttr[name=\"weight\"](%conv1.13)\n",
      "  %1468 : int[] = prim::ListConstruct(%1411, %1411), scope: __module.layer3/__module.layer3.1/__module.layer3.1.conv1\n",
      "  %1469 : int[] = prim::ListConstruct(%1411, %1411), scope: __module.layer3/__module.layer3.1/__module.layer3.1.conv1\n",
      "  %1470 : int[] = prim::ListConstruct(%1411, %1411), scope: __module.layer3/__module.layer3.1/__module.layer3.1.conv1\n",
      "  %1471 : int[] = prim::ListConstruct(%1409, %1409), scope: __module.layer3/__module.layer3.1/__module.layer3.1.conv1\n",
      "  %input.73 : Float(32, 256, 2, 2, strides=[1024, 4, 2, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.71, %weight.133, %1413, %1468, %1469, %1470, %1410, %1471, %1411, %1410, %1410, %1408, %1408), scope: __module.layer3/__module.layer3.1/__module.layer3.1.conv1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %running_var.67 : Tensor = prim::GetAttr[name=\"running_var\"](%bn1.13)\n",
      "  %running_mean.67 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn1.13)\n",
      "  %bias.67 : Tensor = prim::GetAttr[name=\"bias\"](%bn1.13)\n",
      "  %weight.135 : Tensor = prim::GetAttr[name=\"weight\"](%bn1.13)\n",
      "  %input.75 : Float(32, 256, 2, 2, strides=[1024, 4, 2, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.73, %weight.135, %bias.67, %running_mean.67, %running_var.67, %1410, %1415, %1414, %1408), scope: __module.layer3/__module.layer3.1/__module.layer3.1.bn1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:2435:0\n",
      "  %input.77 : Float(32, 256, 2, 2, strides=[1024, 4, 2, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.75), scope: __module.layer3/__module.layer3.1/__module.layer3.1.relu # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:1453:0\n",
      "  %weight.137 : Tensor = prim::GetAttr[name=\"weight\"](%conv2.11)\n",
      "  %1480 : int[] = prim::ListConstruct(%1411, %1411), scope: __module.layer3/__module.layer3.1/__module.layer3.1.conv2\n",
      "  %1481 : int[] = prim::ListConstruct(%1411, %1411), scope: __module.layer3/__module.layer3.1/__module.layer3.1.conv2\n",
      "  %1482 : int[] = prim::ListConstruct(%1411, %1411), scope: __module.layer3/__module.layer3.1/__module.layer3.1.conv2\n",
      "  %1483 : int[] = prim::ListConstruct(%1409, %1409), scope: __module.layer3/__module.layer3.1/__module.layer3.1.conv2\n",
      "  %input.79 : Float(32, 256, 2, 2, strides=[1024, 4, 2, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.77, %weight.137, %1413, %1480, %1481, %1482, %1410, %1483, %1411, %1410, %1410, %1408, %1408), scope: __module.layer3/__module.layer3.1/__module.layer3.1.conv2 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %running_var.69 : Tensor = prim::GetAttr[name=\"running_var\"](%bn2.11)\n",
      "  %running_mean.69 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn2.11)\n",
      "  %bias.69 : Tensor = prim::GetAttr[name=\"bias\"](%bn2.11)\n",
      "  %weight.139 : Tensor = prim::GetAttr[name=\"weight\"](%bn2.11)\n",
      "  %out.11 : Float(32, 256, 2, 2, strides=[1024, 4, 2, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.79, %weight.139, %bias.69, %running_mean.69, %running_var.69, %1410, %1415, %1414, %1408), scope: __module.layer3/__module.layer3.1/__module.layer3.1.bn2 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:2435:0\n",
      "  %input.81 : Float(32, 256, 2, 2, strides=[1024, 4, 2, 1], requires_grad=1, device=cuda:0) = aten::add_(%out.11, %input.71, %1411), scope: __module.layer3/__module.layer3.1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torchvision\\models\\resnet.py:96:0\n",
      "  %input.83 : Float(32, 256, 2, 2, strides=[1024, 4, 2, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.81), scope: __module.layer3/__module.layer3.1/__module.layer3.1.relu # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:1453:0\n",
      "  %1492 : bool = prim::Constant[value=1](), scope: __module.layer4/__module.layer4.0/__module.layer4.0.conv1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %1493 : int = prim::Constant[value=0](), scope: __module.layer4/__module.layer4.0/__module.layer4.0.conv1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %1494 : bool = prim::Constant[value=0](), scope: __module.layer4/__module.layer4.0/__module.layer4.0.conv1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %1495 : int = prim::Constant[value=1](), scope: __module.layer4/__module.layer4.0/__module.layer4.0.conv1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %1496 : int = prim::Constant[value=2](), scope: __module.layer4/__module.layer4.0/__module.layer4.0.conv1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %1497 : NoneType = prim::Constant(), scope: __module.layer4/__module.layer4.0/__module.layer4.0.conv1\n",
      "  %1498 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.layer4/__module.layer4.0/__module.layer4.0.bn1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:2435:0\n",
      "  %1499 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer4/__module.layer4.0/__module.layer4.0.bn1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:2435:0\n",
      "  %_1 : __torch__.torchvision.models.resnet.___torch_mangle_57.BasicBlock = prim::GetAttr[name=\"1\"](%layer4)\n",
      "  %_0 : __torch__.torchvision.models.resnet.___torch_mangle_51.BasicBlock = prim::GetAttr[name=\"0\"](%layer4)\n",
      "  %downsample : __torch__.torch.nn.modules.container.___torch_mangle_50.Sequential = prim::GetAttr[name=\"downsample\"](%_0)\n",
      "  %bn2.13 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_47.BatchNorm2d = prim::GetAttr[name=\"bn2\"](%_0)\n",
      "  %conv2.13 : __torch__.torch.nn.modules.conv.___torch_mangle_46.Conv2d = prim::GetAttr[name=\"conv2\"](%_0)\n",
      "  %relu.15 : __torch__.torch.nn.modules.activation.___torch_mangle_45.ReLU = prim::GetAttr[name=\"relu\"](%_0)\n",
      "  %bn1.15 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_44.BatchNorm2d = prim::GetAttr[name=\"bn1\"](%_0)\n",
      "  %conv1.15 : __torch__.torch.nn.modules.conv.___torch_mangle_43.Conv2d = prim::GetAttr[name=\"conv1\"](%_0)\n",
      "  %weight.141 : Tensor = prim::GetAttr[name=\"weight\"](%conv1.15)\n",
      "  %1509 : int[] = prim::ListConstruct(%1496, %1496), scope: __module.layer4/__module.layer4.0/__module.layer4.0.conv1\n",
      "  %1510 : int[] = prim::ListConstruct(%1495, %1495), scope: __module.layer4/__module.layer4.0/__module.layer4.0.conv1\n",
      "  %1511 : int[] = prim::ListConstruct(%1495, %1495), scope: __module.layer4/__module.layer4.0/__module.layer4.0.conv1\n",
      "  %1512 : int[] = prim::ListConstruct(%1493, %1493), scope: __module.layer4/__module.layer4.0/__module.layer4.0.conv1\n",
      "  %input.85 : Float(32, 512, 1, 1, strides=[512, 1, 1, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.83, %weight.141, %1497, %1509, %1510, %1511, %1494, %1512, %1495, %1494, %1494, %1492, %1492), scope: __module.layer4/__module.layer4.0/__module.layer4.0.conv1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %running_var.71 : Tensor = prim::GetAttr[name=\"running_var\"](%bn1.15)\n",
      "  %running_mean.71 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn1.15)\n",
      "  %bias.71 : Tensor = prim::GetAttr[name=\"bias\"](%bn1.15)\n",
      "  %weight.143 : Tensor = prim::GetAttr[name=\"weight\"](%bn1.15)\n",
      "  %input.87 : Float(32, 512, 1, 1, strides=[512, 1, 1, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.85, %weight.143, %bias.71, %running_mean.71, %running_var.71, %1494, %1499, %1498, %1492), scope: __module.layer4/__module.layer4.0/__module.layer4.0.bn1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:2435:0\n",
      "  %input.89 : Float(32, 512, 1, 1, strides=[512, 1, 1, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.87), scope: __module.layer4/__module.layer4.0/__module.layer4.0.relu # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:1453:0\n",
      "  %weight.145 : Tensor = prim::GetAttr[name=\"weight\"](%conv2.13)\n",
      "  %1521 : int[] = prim::ListConstruct(%1495, %1495), scope: __module.layer4/__module.layer4.0/__module.layer4.0.conv2\n",
      "  %1522 : int[] = prim::ListConstruct(%1495, %1495), scope: __module.layer4/__module.layer4.0/__module.layer4.0.conv2\n",
      "  %1523 : int[] = prim::ListConstruct(%1495, %1495), scope: __module.layer4/__module.layer4.0/__module.layer4.0.conv2\n",
      "  %1524 : int[] = prim::ListConstruct(%1493, %1493), scope: __module.layer4/__module.layer4.0/__module.layer4.0.conv2\n",
      "  %input.91 : Float(32, 512, 1, 1, strides=[512, 1, 1, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.89, %weight.145, %1497, %1521, %1522, %1523, %1494, %1524, %1495, %1494, %1494, %1492, %1492), scope: __module.layer4/__module.layer4.0/__module.layer4.0.conv2 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %running_var.73 : Tensor = prim::GetAttr[name=\"running_var\"](%bn2.13)\n",
      "  %running_mean.73 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn2.13)\n",
      "  %bias.73 : Tensor = prim::GetAttr[name=\"bias\"](%bn2.13)\n",
      "  %weight.147 : Tensor = prim::GetAttr[name=\"weight\"](%bn2.13)\n",
      "  %out.13 : Float(32, 512, 1, 1, strides=[512, 1, 1, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.91, %weight.147, %bias.73, %running_mean.73, %running_var.73, %1494, %1499, %1498, %1492), scope: __module.layer4/__module.layer4.0/__module.layer4.0.bn2 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:2435:0\n",
      "  %_1.11 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_49.BatchNorm2d = prim::GetAttr[name=\"1\"](%downsample)\n",
      "  %_0.11 : __torch__.torch.nn.modules.conv.___torch_mangle_48.Conv2d = prim::GetAttr[name=\"0\"](%downsample)\n",
      "  %weight.149 : Tensor = prim::GetAttr[name=\"weight\"](%_0.11)\n",
      "  %1534 : int[] = prim::ListConstruct(%1496, %1496), scope: __module.layer4/__module.layer4.0/__module.layer4.0.downsample/__module.layer4.0.downsample.0\n",
      "  %1535 : int[] = prim::ListConstruct(%1493, %1493), scope: __module.layer4/__module.layer4.0/__module.layer4.0.downsample/__module.layer4.0.downsample.0\n",
      "  %1536 : int[] = prim::ListConstruct(%1495, %1495), scope: __module.layer4/__module.layer4.0/__module.layer4.0.downsample/__module.layer4.0.downsample.0\n",
      "  %1537 : int[] = prim::ListConstruct(%1493, %1493), scope: __module.layer4/__module.layer4.0/__module.layer4.0.downsample/__module.layer4.0.downsample.0\n",
      "  %input.93 : Float(32, 512, 1, 1, strides=[512, 1, 1, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.83, %weight.149, %1497, %1534, %1535, %1536, %1494, %1537, %1495, %1494, %1494, %1492, %1492), scope: __module.layer4/__module.layer4.0/__module.layer4.0.downsample/__module.layer4.0.downsample.0 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %running_var.75 : Tensor = prim::GetAttr[name=\"running_var\"](%_1.11)\n",
      "  %running_mean.75 : Tensor = prim::GetAttr[name=\"running_mean\"](%_1.11)\n",
      "  %bias.75 : Tensor = prim::GetAttr[name=\"bias\"](%_1.11)\n",
      "  %weight.151 : Tensor = prim::GetAttr[name=\"weight\"](%_1.11)\n",
      "  %identity : Float(32, 512, 1, 1, strides=[512, 1, 1, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.93, %weight.151, %bias.75, %running_mean.75, %running_var.75, %1494, %1499, %1498, %1492), scope: __module.layer4/__module.layer4.0/__module.layer4.0.downsample/__module.layer4.0.downsample.1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:2435:0\n",
      "  %input.95 : Float(32, 512, 1, 1, strides=[512, 1, 1, 1], requires_grad=1, device=cuda:0) = aten::add_(%out.13, %identity, %1495), scope: __module.layer4/__module.layer4.0 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torchvision\\models\\resnet.py:96:0\n",
      "  %input.97 : Float(32, 512, 1, 1, strides=[512, 1, 1, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.95), scope: __module.layer4/__module.layer4.0/__module.layer4.0.relu # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:1453:0\n",
      "  %bn2 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_56.BatchNorm2d = prim::GetAttr[name=\"bn2\"](%_1)\n",
      "  %conv2 : __torch__.torch.nn.modules.conv.___torch_mangle_55.Conv2d = prim::GetAttr[name=\"conv2\"](%_1)\n",
      "  %relu : __torch__.torch.nn.modules.activation.___torch_mangle_54.ReLU = prim::GetAttr[name=\"relu\"](%_1)\n",
      "  %bn1 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_53.BatchNorm2d = prim::GetAttr[name=\"bn1\"](%_1)\n",
      "  %conv1 : __torch__.torch.nn.modules.conv.___torch_mangle_52.Conv2d = prim::GetAttr[name=\"conv1\"](%_1)\n",
      "  %weight.153 : Tensor = prim::GetAttr[name=\"weight\"](%conv1)\n",
      "  %1552 : int[] = prim::ListConstruct(%1495, %1495), scope: __module.layer4/__module.layer4.1/__module.layer4.1.conv1\n",
      "  %1553 : int[] = prim::ListConstruct(%1495, %1495), scope: __module.layer4/__module.layer4.1/__module.layer4.1.conv1\n",
      "  %1554 : int[] = prim::ListConstruct(%1495, %1495), scope: __module.layer4/__module.layer4.1/__module.layer4.1.conv1\n",
      "  %1555 : int[] = prim::ListConstruct(%1493, %1493), scope: __module.layer4/__module.layer4.1/__module.layer4.1.conv1\n",
      "  %input.99 : Float(32, 512, 1, 1, strides=[512, 1, 1, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.97, %weight.153, %1497, %1552, %1553, %1554, %1494, %1555, %1495, %1494, %1494, %1492, %1492), scope: __module.layer4/__module.layer4.1/__module.layer4.1.conv1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %running_var.77 : Tensor = prim::GetAttr[name=\"running_var\"](%bn1)\n",
      "  %running_mean.77 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn1)\n",
      "  %bias.77 : Tensor = prim::GetAttr[name=\"bias\"](%bn1)\n",
      "  %weight.155 : Tensor = prim::GetAttr[name=\"weight\"](%bn1)\n",
      "  %input.101 : Float(32, 512, 1, 1, strides=[512, 1, 1, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.99, %weight.155, %bias.77, %running_mean.77, %running_var.77, %1494, %1499, %1498, %1492), scope: __module.layer4/__module.layer4.1/__module.layer4.1.bn1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:2435:0\n",
      "  %input.103 : Float(32, 512, 1, 1, strides=[512, 1, 1, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.101), scope: __module.layer4/__module.layer4.1/__module.layer4.1.relu # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:1453:0\n",
      "  %weight.157 : Tensor = prim::GetAttr[name=\"weight\"](%conv2)\n",
      "  %1564 : int[] = prim::ListConstruct(%1495, %1495), scope: __module.layer4/__module.layer4.1/__module.layer4.1.conv2\n",
      "  %1565 : int[] = prim::ListConstruct(%1495, %1495), scope: __module.layer4/__module.layer4.1/__module.layer4.1.conv2\n",
      "  %1566 : int[] = prim::ListConstruct(%1495, %1495), scope: __module.layer4/__module.layer4.1/__module.layer4.1.conv2\n",
      "  %1567 : int[] = prim::ListConstruct(%1493, %1493), scope: __module.layer4/__module.layer4.1/__module.layer4.1.conv2\n",
      "  %input.105 : Float(32, 512, 1, 1, strides=[512, 1, 1, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.103, %weight.157, %1497, %1564, %1565, %1566, %1494, %1567, %1495, %1494, %1494, %1492, %1492), scope: __module.layer4/__module.layer4.1/__module.layer4.1.conv2 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458:0\n",
      "  %running_var : Tensor = prim::GetAttr[name=\"running_var\"](%bn2)\n",
      "  %running_mean : Tensor = prim::GetAttr[name=\"running_mean\"](%bn2)\n",
      "  %bias.79 : Tensor = prim::GetAttr[name=\"bias\"](%bn2)\n",
      "  %weight.159 : Tensor = prim::GetAttr[name=\"weight\"](%bn2)\n",
      "  %out : Float(32, 512, 1, 1, strides=[512, 1, 1, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.105, %weight.159, %bias.79, %running_mean, %running_var, %1494, %1499, %1498, %1492), scope: __module.layer4/__module.layer4.1/__module.layer4.1.bn2 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:2435:0\n",
      "  %input.107 : Float(32, 512, 1, 1, strides=[512, 1, 1, 1], requires_grad=1, device=cuda:0) = aten::add_(%out, %input.97, %1495), scope: __module.layer4/__module.layer4.1 # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torchvision\\models\\resnet.py:96:0\n",
      "  %input.109 : Float(32, 512, 1, 1, strides=[512, 1, 1, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.107), scope: __module.layer4/__module.layer4.1/__module.layer4.1.relu # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:1453:0\n",
      "  %1576 : int = prim::Constant[value=1](), scope: __module.avgpool # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:1213:0\n",
      "  %1577 : int[] = prim::ListConstruct(%1576, %1576), scope: __module.avgpool\n",
      "  %x : Float(32, 512, 1, 1, strides=[512, 1, 1, 1], requires_grad=1, device=cuda:0) = aten::adaptive_avg_pool2d(%input.109, %1577), scope: __module.avgpool # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\functional.py:1213:0\n",
      "  %890 : int = prim::Constant[value=1]() # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torchvision\\models\\resnet.py:273:0\n",
      "  %891 : int = prim::Constant[value=-1]() # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torchvision\\models\\resnet.py:273:0\n",
      "  %input : Float(32, 512, strides=[512, 1], requires_grad=1, device=cuda:0) = aten::flatten(%x, %890, %891) # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torchvision\\models\\resnet.py:273:0\n",
      "  %bias : Tensor = prim::GetAttr[name=\"bias\"](%fc)\n",
      "  %weight : Tensor = prim::GetAttr[name=\"weight\"](%fc)\n",
      "  %1581 : Float(32, 10, strides=[10, 1], requires_grad=1, device=cuda:0) = aten::linear(%input, %weight, %bias), scope: __module.fc # E:\\pytorch-course\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114:0\n",
      "  return (%1581)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "images = images.to(device)\n",
    "labels = labels.to(device)\n",
    "grid = torchvision.utils.make_grid(images)\n",
    "writer.add_image('images', grid, 0)\n",
    "writer.add_graph(model, images)\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "num_train_data = len(train_dataset)\n",
    "num_valid_data = len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc epoch: 0: 0.7311999797821045\n",
      "valid_acc epoch: 0: 0.8537999987602234\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for epoch in range (0, epochs):\n",
    "    model.train()\n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "        checkpoint = {\n",
    "            'model_state' : model.state_dict(),\n",
    "            'optim_state' : optimizer.state_dict(),\n",
    "            'epoch' : epoch\n",
    "        }\n",
    "\n",
    "    torch.save(checkpoint, 'chechpoint.pth')\n",
    "\n",
    "    correct_train = 0\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        yhat = model(x)\n",
    "        _, train_label = torch.max(yhat, 1)\n",
    "\n",
    "        correct_train += (train_label == y).sum()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            loss = criterion(yhat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    train_acc = correct_train / num_train_data\n",
    "    \n",
    "    writer.add_scalar('Loss/train', loss, epoch)\n",
    "    writer.add_scalar('Accuracy/train', train_acc, epoch)\n",
    "    \n",
    "    print(f'train_acc epoch: {epoch}: {train_acc}')\n",
    "\n",
    "    model.eval()\n",
    "    correct_valid = 0\n",
    "\n",
    "    for x_val, y_val in valid_loader:\n",
    "        x_val = x_val.to(device)\n",
    "        y_val = y_val.to(device)\n",
    "            \n",
    "        yhat_val = model(x_val)\n",
    "        _, yhat_label = torch.max(yhat_val, 1)\n",
    "        \n",
    "        correct_valid += (yhat_label == y_val).sum()\n",
    "\n",
    "    valid_acc = correct_valid / num_valid_data\n",
    "\n",
    "    writer.add_scalar('Accuracy/valid', valid_acc, epoch)\n",
    "\n",
    "    print(f'valid_acc epoch: {epoch}: {valid_acc}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
